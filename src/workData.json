{
  "data": [
    {
      "name": "Electronicos Fantasticos",
      "tags": [
        "hardware",
        "electronics",
        "music",
        "sound",
        "technology",
        "performance",
        "arts"
      ],
      "date": "June - August 2022",
      "technology": "Hardware, Electronics, CRTs, Amplifiers",
      "description": [
        {
          "text": "After being awarded the NYU Abu Dhabi summer internship grant of 3000 USD, I set out to Tokyo to work with Ei Wada and his Electronicos Fantasticos project. Electronicos Fantasticos is a project that aims “to reincarnate old electrical appliances as new electromagnetic instruments, invents new ways to play music, and co-create orchestras and festivals with diverse people and artist/musician Ei Wada.” I helped the team to perform at the Fuji Rock’ 22 Music Festival as well."
        },
        {
          "text": "Initially we were preparing to perform a version of the Bon Odori dance at the Fuji Rock festival. My work involved assisting with all the electronics at this time and there were instances I would also improvise on music compositions with the team. For Fuji Rock, we traveled with all our equipment from Tokyo to Naeba Ski Resort, in the Niigata prefecture of Japan. As our performance was late at night, we spent quite a bit of the evening setting up for the performance. "
        },
        {
          "text": "It was a great and learning experience helping out with the preparation for Fuji Rock. After the team got done with Fuji Rock, I got involved more in the technical aspects of the project and started creating instruments like the barcode scanner or the electric fan harp. Even after leaving Japan, I have still been in touch with Wada-san and the rest of the team. We have also set-up a “WORLDWIDE Orchest Lab” to bring together musicians globally who are interested in creating instruments like the ones Electronicos Fantasticos makes. "
        }
      ],
      "media": [
        {
          "link": "https://i.imgur.com/lkCgUfP.png",
          "type": "image",
          "caption": "Us preparing for a Bon Odori performance at Fuji Rock"
        },
        {
          "link": "https://i.imgur.com/hvSah5s.png",
          "type": "image",
          "caption": "A meeting at the shrine to combine tradtional instruments with electornic"
        },
        {
          "link": "https://www.youtube.com/embed/m4FkYmwpMYE",
          "type": "video",
          "caption": "A short improvisation with me on the Electric Fan Harp"
        },
        {
          "link": "https://i.imgur.com/mrkPhbK.png",
          "type": "image",
          "caption": "Creating a barcode scanner instrument"
        },
        {
          "link": "https://www.youtube.com/embed/uip466i8Kec",
          "type": "video",
          "caption": "Playing the barcode scanner I created"
        },
        {
          "link": "https://i.imgur.com/aNTQplc.jpg",
          "type": "image",
          "caption": "A CRTelecaster we worked on"
        },
        {
          "link": "https://www.youtube.com/embed/OakYFQBiFcY",
          "type": "video",
          "caption": "Exploring the sounds of a new electronic device with Wada-san"
        },
        {
          "link": "https://i.imgur.com/6hj4zGn.png",
          "type": "image",
          "caption": "Experimenting with the Electric Fan Harp's light system"
        },
        {
          "link": "https://www.youtube.com/embed/M-05hiOEqC8",
          "type": "video",
          "caption": "Setting up before the Fuji Rock performance"
        },
        {
          "link": "https://www.youtube.com/embed/IqBaU5mhl2I",
          "type": "video",
          "caption": "Combining both skates and barcodes together"
        }
      ]
    },
    {
      "name": "Faceshopping",
      "tags": [
        "augmented reality",
        "audio reactive",
        "music",
        "technology",
        "new media",
        "identity",
        "transmedia",
        "visualizers"
      ],
      "date": "December 2022",
      "technology": "Cinema4D, Meta Spark Studio",
      "sourceCode": "https://github.com/AakSin/Faceshopping",
      "liveLink": "https://www.instagram.com/ar/1794320447605221/",
      "description": [
        {
          "text": "Faceshopping is an instagram filter that recreates SOPHIE’s music video for Faceshopping in augmented reality. The music video covers themes of body dysmorphia especially with the lens of SOPHIE being a transgender individual. By turning the camera at the user, I aim to put them in SOPHIE’s place. The filter was presented in Berlin, Germany at “MANIFEST:IO – The Symposium for New Media & Electronic Art.”"
        },
        {
          "header": "Conception",
          "text": "SOPHIE’s Faceshopping music video has stuck with me ever since I saw it. It contains visuals of SOPHIE’s face being distorted while loud abrasive hyperpop/PC-music blares through. For this project, I wanted people to be able to experience these distortions and music on their face. I looked into a lot of AR platforms like Unity, Vuforia or Styly but realized Meta had created a pretty good solution for face filters."
        },
        {
          "header": "Technical Implementation",
          "text": "I imported the model of the face that Meta provides for the filter into Cinema4D. I added a PoseMorph tag to it and then set up keyframes with 6 different distortions. I exported this model into Meta Spark Studio. I used the frequency splitter node in Spark AR’s node graph then and passed SOPHIE”s Faceshopping through it. The middle frequencies were mapped to the intensity of the light and the back wall. The high frequencies were mapped to the direction and movement of the light. I wrote a simple javascript script after that to animate and cycle through all the distortions periodically and in a loop."
        },
        {
          "header": "Exhibition",
          "text": "The filter was part of TimeLab’s, a transmedia laboratory for artistic research in Berlin, MANIFEST:IO Symposium for New Media and Electronic Art in Berlin. I printed the poster pictured above and hung it around the venue. The work was exhibited among other transmedia and new media artists like TouchDesigner YoutTuber Bileam Tschepe (Elekktronaut). The exhibition occurred at Alte Muenze, a popular night club and an art venue in Berlin."
        }
      ],
      "media": [
        {
          "link": "https://i.imgur.com/3KTBwho.png",
          "type": "image",
          "caption": "The poster for Faceshopping at the MANIFEST:IO exhibition"
        },
        {
          "link": "https://www.youtube.com/embed/_eCXYYLGoY0",
          "type": "video",
          "caption": "A demo of the filter on my face"
        },
        {
          "link": "https://i.imgur.com/gSKyMFQ.png",
          "type": "image",
          "caption": "Cinema4D project file screenshot"
        },
        {
          "link": "https://i.imgur.com/7oDPt7f.png",
          "type": "image",
          "caption": "Meta Spark Studio file screenshot"
        },
        {
          "link": "https://i.imgur.com/MXHY3nO.jpg",
          "type": "image",
          "caption": "A still from the Faceshopping music video by SOPHIE"
        },
        {
          "link": "https://i.imgur.com/E7YezsD.png",
          "type": "image",
          "caption": "An in-app render of the filter"
        }
      ]
    },
    {
      "name": "aloegarten",
      "tags": [
        "virtual reality",
        "music",
        "new media",
        "immersive",
        "transmedia",
        "technology",
        "arts",
        "identity",
        "visualizers"
      ],
      "date": "October - November 2022",
      "technology": "Unity, C#, Cinema4D",
      "description": [
        {
          "text": "Aloegarten is a virtual reality recreation of the worlds of Swedish artist Ecco2k / Zak Arogundade. Themes from his works regarding colorist societal alienation, substance abuse, powerlessness and identity are extrapolated in this three dimensional transmedia project.The project covers two of his songs - Peroxide and AAA Powerline."
        },
        {
          "header": "Conception",
          "text": "The themes present in Ecco2k’s works are very personal to my lives as well. Through an exploration of his music videos and lyricism, I chose to expand and interpret these themes into my understanding of them. This started out by creating conception sketches in Procreate (present in the gallery above). This was to gain an understanding of the feel of the scenes. Then I started mapping out the project in 3D."
        },
        {
          "header": "Technical Implementation",
          "text": "The project is created using Unity. For the water in the first/Peroxide scene, I use Unity’s Universal Render Pipeline (URP). Using displacement maps and shadergraphs, I was able to simulate wave-like motion for the water. I then wrote C# scripts that would track this motion and create a buoyant force on any rigid body floating on water. The raft in the scene was given this script along with a controller script. A portal was implemented to take the user to the second/AAA Powerline scene."
        },
        {
          "text": "In the AAA Powerline scene, I place the viewer into a car and create a cyberpunk city around them. I then start tweaking the material of the glass so that it simulates refraction and distortion of perspectives when viewed from different angles. The person is unable to move in the scene. They are in the backseat and unable to control their actions. I create a holographic material for the VR hands of the person, which they have been detached from. The car continues moving in an endless loop, unable to be controlled and having you forever stuck."
        }
      ],
      "media": [
        {
          "link": "https://i.imgur.com/6DDd2DA.png",
          "type": "image",
          "caption": "A still from the Peroxide scene"
        },
        {
          "link": "https://i.imgur.com/og7BxpX.jpg",
          "type": "image",
          "caption": "A still from the AAA Powerline scene"
        },
        {
          "link": "https://www.youtube.com/embed/w7fjQeXgBbU",
          "type": "video",
          "caption": "A full walkthrough of aloegarten"
        },
        {
          "link": "https://i.imgur.com/XfSOhui.png",
          "type": "image",
          "caption": "Conception sketches for AAA Powerline scene"
        },
        {
          "link": "https://i.imgur.com/h922y93.png",
          "type": "image",
          "caption": "Conception sketches for a deleted scene"
        },
        {
          "link": "https://i.imgur.com/VEHphL9.png",
          "type": "image",
          "caption": "A screenshot of the shader graph for the water"
        },
        {
          "link": "https://i.imgur.com/A3GN9Ns.png",
          "type": "image",
          "caption": "A still from the Peroxide scene"
        },
        {
          "link": "https://i.imgur.com/udxh6DE.jpg",
          "type": "image",
          "caption": "A still from the AAA Powerline scene"
        }
      ]
    },
    {
      "name": "Communication Plateau",
      "tags": [
        "audio reactive",
        "installation",
        "light",
        "sound",
        "technology",
        "visualizers"
      ],
      "date": "December 2022",
      "technology": "Kinect, TouchDesigner",
      "sourceCode": "https://github.com/AakSin/CommunicationPlateau",
      "description": [
        {
          "text": "Communication Plateau is an interactive sound and light installation derived from Plato’s Allegory of The Cave. Inspired by the imagery of projections on the wall as a metaphor for existence, I set out to simulate a similar visual language in this installation. The project involves a Kinect Azure pointed at the entrance of the exhibition. The Kinect then sends point cloud data into TouchDesigner. The point cloud data is then formatted and passed through various nodes like blur, feedback and threshold. These nodes are also mapped to an original soundtrack and depending on the frequency, the values in the node changes, creating an audio-reactive experience."
        }
      ],
      "media": [
        {
          "link": "https://i.imgur.com/RTQ4MOq.jpg",
          "type": "image",
          "caption": "Picture from project installation"
        },
        {
          "link": "https://www.youtube.com/embed/A8lEnJhtTaM",
          "type": "video",
          "caption": "First demo on screen"
        },
        {
          "link": "https://www.youtube.com/embed/5oVOlH9Gxgw",
          "type": "video",
          "caption": "First demo on a projector"
        },
        {
          "link": "https://www.youtube.com/embed/ihP8nLv5nsE",
          "type": "video",
          "caption": "Video from project installation"
        }
      ]
    },
    {
      "name": "“Real” Art",
      "tags": ["technology", "installation", "artificial intelligence", "web"],
      "date": "November 2022",
      "technology": "JavaScript, React, Next.js, Firebase",
      "sourceCode": "https://github.com/AakSin/realArt",
      "liveLink": "https://louvread-realart.vercel.app/",
      "description": [
        {
          "header": "Context",
          "text": "“Real” Art was created in collaboration with Louvre Abu Dhabi and was part of a weekend installation there. The initial prompt provided to us was, “What does it mean to be human?”. This was a time when conversations regarding AI art, DALL-E, ownership etc. were quite prevalent. In the context of this situation, we decided to create a project that would explore whether people could even sense the presence of a “real” human artist in artworks anymore."
        },
        {
          "header": "Concept",
          "text": "The concept revolves around presenting two images to a user side-by-side. One of them present at the Louvre Abu Dhabi as of now and another that was generated by an AI image generator (DALL-E). After selecting their answers, the cards containing the image would flip. The AI card would reveal the prompt used to generate the image. The real card would reveal the name of the artwork, artist and year of creation. The users could also see a voting tally at the bottom, to see how they compared against other users."
        },
        {
          "header": "AI Image Generation",
          "text": "This part of the project had some unexpected challenges. As seen in the initial renders in the screenshots above, the AI would have glitch artifacts, issues with human limbs or minor details in nature would be wrong. Researching the schools of art, the painters and their styles, the subjects etc. helped us get to prompts that would render a similar picture."
        },
        {
          "header": "Technical Implementation",
          "text": "The front end is created using Next.js. It was created to be displayed on a large touchscreen display along with multiple iPads spread across the site of install. The backend involves a Firebase storage saving the number of votes each painting has garnered along with the rest of the painting’s metadata. There are then API calls being done using Next.js to handle retrieval and incrementation of votes."
        },
        {
          "header": "Exhibition",
          "text": "After installing the project in the Louvre Abu Dhabi premises, we started inviting people to take the quiz. We’d explain what the project is, what AI art is and then let people take the quiz. Throughout the quiz and after the quiz we’d generate discussions among groups of people regarding how they are telling the AI artwork apart from the real artwork, the ethics of AI art and what does the future of the art landscape look like with the introduction of AI."
        }
      ],
      "media": [
        {
          "link": "https://i.imgur.com/goT2vNA.png",
          "type": "image",
          "caption": "Screenshot of project website"
        },
        {
          "link": "https://i.imgur.com/zQ4ZggW.jpg",
          "type": "image",
          "caption": "Renders by the AI"
        },
        {
          "link": "https://i.imgur.com/3WsBBKZ.jpg",
          "type": "image",
          "caption": "Renders by the AI"
        },
        {
          "link": "https://www.youtube.com/embed/xePH6z5mIQ8",
          "type": "video",
          "caption": "Overview of \"Real\" Art Installation Setup"
        },
        {
          "link": "https://www.youtube.com/embed/psr9Tbsioxw",
          "type": "video",
          "caption": "Audience Interaction at \"Real\" Art"
        },
        {
          "link": "https://www.youtube.com/embed/16X2hZOTZp0",
          "type": "video",
          "caption": "Audience Interaction at \"Real\" Art"
        }
      ]
    },
    {
      "name": "CamJam",
      "tags": ["technology", "sound", "music", "interconnected", "web"],
      "date": "May 2022",
      "technology": "JavaScript, ml5.js,p5.js,WebSockets",
      "sourceCode": "https://github.com/AakSin/CamJam",
      "liveLink": "https://camjam1.glitch.me/",
      "description": [
        {
          "text": "CamJam is a web-based multiplayer platform wherein users can play instruments like the piano, guitar, bass and drums just by using their webcam. A person can create a room and then share that link with up to 3 more people. Everyone can then select their instruments and start jamming together."
        },
        {
          "header": "Concept",
          "text": "During COVID-19, I came to realize how much I missed playing music together with my friends. I initially started out looking for a way to jam with real instruments over the web but then I started thinking about people who don’t own instruments. With this project, I set out to provide a platform where people could jam together with their friends by just using their webcams (something that became a necessity during COVID’s Zoom era)."
        },
        {
          "header": "Instrument Implementation",
          "text": "The piano works on ml5.js’s HandPose model. It gets the coordinate point of each joint in the hand and then depending on which finger was brought down, plays a certain note. The bass and drums work on ml5.js’s PoseNet model. Depending on how much movement the left and right wrists had in certain sections of the screen, a certain sound is played. The guitar combines both HandPose and PoseNet. The left hand keeps track of which finger is down and which note to play. The right hand tracks the wrist to check when a person brings it down, which equates to a strum."
        },
        {
          "header": "Server Implementation",
          "text": "The site uses p5’s Live Media implementation (https://github.com/vanevery/p5LiveMedia). The  user’s webcam is captured using the createCapture method. The other webcams are provided as streams and are “drawn onto canvas”. To pass sound information around, I used the provided getData method. Depending on whichever sound had to be played, a certain string would be passed around. This would trigger the sound in all clients except the creator of the sound."
        }
      ],
      "media": [
        {
          "link": "https://i.imgur.com/C6RpsJy.png",
          "type": "image",
          "caption": "Screenshot of website layout"
        },
        {
          "link": "https://github.com/AakSin/aaksin-public/raw/main/camjam/guitar.gif",
          "type": "image",
          "caption": "Guitar tech demo"
        },
        {
          "link": "https://github.com/AakSin/aaksin-public/raw/main/camjam/b65f157f0372bc4f5f68f41e459935d2.gif",
          "type": "image",
          "caption": "Piano tech demo"
        },
        {
          "link": "https://github.com/AakSin/aaksin-public/raw/main/camjam/drums.gif",
          "type": "image",
          "caption": "Drum tech demo"
        },
        {
          "link": "https://github.com/AakSin/aaksin-public/raw/main/camjam/bass.gif",
          "type": "image",
          "caption": "Bass tech demo"
        },
        {
          "link": "https://www.youtube.com/embed/2_LWjRFyWSs?",
          "type": "video",
          "caption": "In-person demo of CamJam"
        }
      ]
    },
    {
      "name": "Genesis",
      "tags": [
        "technology",
        "virtual reality",
        "music",
        "hardware",
        "electronics",
        "new media",
        "visualizers"
      ],
      "date": "April-May 2022",
      "technology": "A-Frame, three.js, JavaScript, Arduino, C++, XBee, WebSockets",
      "sourceCode": "https://github.com/AakSin/IntroToIM/tree/main/finalProject",
      "liveLink": "https://aframesockets.glitch.me/",
      "description": [
        {
          "text": "Genesis is a webVR music experience set to Genesis by Grimes. The experience involves the user suspended in mid-air as particles revolve around them. With a custom Arduino controller made by us, the user could manipulate these particles and move around. The aim was to create a surreal and psychedelic visualizer to Grimes’ track while providing the user complete immersion and control."
        },
        {
          "text": "Note: The live demo is paired with a custom Arduino controller to control the visuals. Even without it though, a user can still enjoy the visuals in VR."
        },
        {
          "header": "Hardware Implementation",
          "text": "The controller involves an Arduino Uno housing an accelerometer, two buttons and an XBee module. Since the visuals of the project involve three dimensions, we wanted a sensor that would also give us inputs in three dimensions. Hence, we chose the accelerometer to give us the user’s tilt and motion in the three axes. The buttons are simply to move up and down in the virtual world. The XBee module was crucial to make the controller wireless. Initial prototypes involved the project with a wire and that would severely restrict the controller’s motion."
        },
        {
          "header": "Website Implementation",
          "text": "The virtual world is set up using A-Frame, a webVR framework built on top of three.js. I set up an animation for the spheres to move in a spiral pattern by iterating through all of them and changing their transformations. The input from the controller then controls the radius of this spiral, the movement speed of the spheres and the color of the spheres. Using spatial audio, as the user descends down into the spiral, the audio of Genesis becomes louder."
        },
        {
          "text": "As we didn’t own a virtual reality headset at that time, we decided to use a phone inside of Google Cardboard as our virtual reality headset. The XBee module only connects with a computer though and thus I had to set up an entire WebSocket system to communicate data between the phone and the laptop. Upon every change of input received by the laptop, the data would be sent to all other clients who have the site open. This included the phone, hence allowing the phone to mirror the laptop’s visuals."
        }
      ],
      "media": [
        {
          "link": "https://github.com/AakSin/aaksin-public/raw/main/genesis/3498ba743b114470404ecbc99d13ce0d.gif",
          "type": "image",
          "caption": "Genesis Device Demo"
        },
        {
          "link": "https://www.youtube.com/embed/ui-yJ7_DYvo",
          "type": "video",
          "caption": "In-class presentation of Genesis"
        },
        {
          "link": "https://www.youtube.com/embed/ELKH0ijsJAQ",
          "type": "video",
          "caption": "Exhibition presentation of Genesis"
        },
        {
          "link": "https://www.youtube.com/embed/u7Tf66VZSUg",
          "type": "video",
          "caption": "Prototyping Genesis V1"
        },
        {
          "link": "https://www.youtube.com/embed/jX-XVjoRggI",
          "type": "video",
          "caption": "Prototyping Genesis V2"
        },
        {
          "link": "https://i.imgur.com/4poHKGx.jpg",
          "type": "image",
          "caption": "Picture of Genesis Controller"
        }
      ]
    },
    {
      "name": "Glitch Princess",
      "tags": [
        "technology",
        "audio reactive",
        "music",
        "new media",
        "arts",
        "visualizers"
      ],
      "date": "March 2022",
      "technology": "p5.js, JavaScript, Procreate",
      "sourceCode": "https://github.com/AakSin/IntroToIM/tree/main/week5",
      "liveLink": "https://aaksin.github.io/IntroToIM/week5/game",
      "description": [
        {
          "text": "Glitch Princess is an audio-reactive video game set to Pocky Boy by Yeule. It involves a hand-drawn sprite of Yeule floating around in space while planets spawn to musical triggers. The player has to keep dodging these planets or their health lowers. The game finishes once the health reaches zero or the song finishes."
        },
        {
          "header": "Technical Implementation",
          "text": "The background of the game is an audio-reactive star system which changes throughout the song. The planets are of three kinds - static, hand animated and procedurally generated. Each planet regardless, is also assigned random attributes like speed, size and trajectory."
        },
        {
          "text": "The procedurally generated planets involve creating generative art using p5 and then applying it to circles using createGraphic(). The planets appear to be in motion using a procedurally generated smoke mask created by me. The planet spawn event is triggered upon crossing a certain frequency threshold. The trigger rate however changes depending on which part of the song we are on, as during the high energy parts I have applied limiters to not overcrowd the player with planets."
        }
      ],
      "media": [
        {
          "link": "https://i.imgur.com/Ljfu1JA.png",
          "type": "image",
          "caption": "Still from Glitch Princess"
        },
        {
          "link": "https://www.youtube.com/embed/6FN5UfUDAFQ",
          "type": "video",
          "caption": "Playthrough of Glitch Princess"
        },

        {
          "link": "https://camo.githubusercontent.com/780e237e682aaf6cd61613fbb9310d4cb06f9081cfeaeedda77dfb5359702204/68747470733a2f2f692e696d6775722e636f6d2f455241415576782e6a7067",
          "type": "image",
          "caption": "Conception of the game on paper"
        },
        {
          "link": "https://i.imgur.com/LHG0FIM.jpg",
          "type": "image",
          "caption": "Initial sprite drawn in Procreate"
        },
        {
          "link": "https://github.com/AakSin/IntroToIM/raw/main/week5/assets/asteroid.png",
          "type": "image",
          "caption": "Spritesheet for the hand drawn planet"
        }
      ]
    }
  ]
}
