{
  "data": [
    {
      "name": "NEEEU",
      "tags": [
        "hardware",
        "electronics",
        "music",
        "sound",
        "technology",
        "performance",
        "arts",
        "game engine"
      ],
      "date": "May - July 2023",
      "technology": "Meta Spark Studio, Unity, GLSL, Cinema4D, Figma",
      "description": [
        {
          "text": "During the summer of 2023, I undertook a two-month internship at NEEEU Spaces, a Berlin-based creative technology and design agency. In my capacity as a creative technologist, I actively contributed to a wide variety of projects. These included the development of a Unity prototype for a spatial audio-based game, the creation of character concepts in Cinema4D for a chat LLM-based application, and the finalization of responsive layouts in Figma for a museum installation. My biggest project however was working on an augmented reality advertisement for Sigur Rós affiliated perfume brand Fischersund, to be published in BMW’s yearly lifestyle magazine, Freude Forever."
        },
        {
          "header": "Fredue Forever Flower",

          "text": "The project started out with Fischersund sending out an initial concept and a render to us. The render involved this abstract floral entity surrounded with pearls to reflect the ethos behind Fischersund - creating scents based on memories and feelings rather than corporeal objects. With the creative director of NEEU, Raffael Moco Schiller, I set up a plan to create an Instagram filter using Meta Spark studio that would involve a series of animations going off and forming the flower when someone scans the flower’s picture in the magazine."
        },
        {
          "text": "First, we set off to create a 2D ripple shader using Meta Spark’s node system. Then due to the memory/size limitations of creating a graphic-based application as an Instagram filter, we had to use a lot of workarounds. For example, to create the illusions of pearls I set up a billboard shader of flat circles which would track the rotation of the phone, hence giving the illusion of a sphere. After that, I had to write another shader that would mimic the refraction that a sphere does onto a flat circle by converting coordinates from world space to camera space and then passing the normal texture of a sphere into this calculation."
        },
        {
          "text": "Bringing together the final shader involved finally animating the elements of the original flower sent to us by Fischersund. The stalks were animated to grow in Meta Spark Studio while to animate petal movement, we rigged them with bones and then set off animation clips that get triggered after the flower is formed. After prototyping this implementation and making it smoother and less buggy for Instagram, we handed this over to Fischersund who presented this in BMW’s magazine along with a render on <a href=\"https://www.bmw.com/en/magazine/freudeforever/freude-forever-magazine-issue1-seven-senses.html\" \" target=\"_blank\"> their website. </a> "
        }
      ],
      "media": [
        {
          "link": "../images/neeeu/1.jpeg",
          "type": "image",
          "caption": "Initial flower render sent from Fishcersund to NEEEU"
        },
        {
          "link": "../images/neeeu/2.jpeg",
          "type": "image",
          "caption": "Still from prototype"
        },
        {
          "link": "https://www.youtube.com/embed/aY_jOdWfwmY",
          "type": "video",
          "caption": "Final filter published on Instagram"
        },
        {
          "link": "https://www.youtube.com/embed/IKPO6GcgdB8",
          "type": "video",
          "caption": "Final stylized render put on BMW website"
        },

        {
          "link": "../images/neeeu/3.jpeg",
          "type": "image",
          "caption": "Concept charcter made in Cinema4D"
        },
        {
          "link": "../images/neeeu/4.jpeg",
          "type": "image",
          "caption": "Concept charcter made in Cinema4D"
        },
        {
          "link": "../images/neeeu/5.png",
          "type": "image",
          "caption": "Poster/still from Purple Friday (spatial audio game)"
        }
      ]
    },
    {
      "name": "To Water A Dying Garden",
      "tags": [
        "hardware",
        "electronics",
        "interconnected",
        "immersive",
        "technology",
        "new media",
        "arts"
      ],
      "date": "November - December 2023",
      "technology": "TouchDesigner, Arduino",
      "description": [
        {
          "text": "\"To Water A Dying Garden\" is an immersive installation exploring the interplay of nature, technology, and human influence. Informed by Hegelian notions of nature as the unfolding of absolute spirit, the installation transforms dissonant point clouds into a vibrant garden as you lift a teapot and slowly water a solitary plant. This act mirrors Hegel's concept of simultaneous negation and preservation. As you nurture, the garden blossoms temporarily, symbolizing life's transient beauty and echoing Hegel's thoughts on the evolving nature of existence. To enhance the experience, another participant can shine light with their phone's flashlight on the growing garden, illuminating the collaborative dance between light and life."
        },
        {
          "header": "Technical Implementation",

          "text": "The project operates using photoresisotrs, self-made switches, Arduino, and TouchDesigner. The first trigger is when the pot gets lifted up, a signal is sent to TouchDesigner spawning a new point cloud flower in the garden. This is done by a replicator system that instantiates a custom TouchDesigner component with random values (for color, size, position) that is linked to a new Geo node. Once you start watering the plant, the pointclouds start coming back together to form flowers by switching between two point cloud files. The photoresistors are then mapped to the Bloom TOP, deciding the threshold of the glow depening on how much light the plant receives. The point clouds when dispersed are in constant noise based movement, along with the camera. Future implementations of this installation in a bigger room, would involve tracking camera and motion to the movements of the crowd."
        }
      ],
      "media": [
        {
          "link": "../images/toWater/1.jpg",
          "type": "image",
          "caption": "Audience interaction with installation"
        },
        {
          "link": "https://www.youtube.com/embed/N5dPNEdEnvE",
          "type": "video",
          "caption": "Audience using the water prop to bring the point clouds together"
        },
        {
          "link": "https://www.youtube.com/embed/kRAiUUrN4h8",
          "type": "video",
          "caption": "Documentation video from show"
        },
        {
          "link": "https://www.youtube.com/embed/bfNiqKcE3A4",
          "type": "video",
          "caption": "Audience interaction"
        },
        {
          "link": "https://www.youtube.com/embed/SopnBFCmEtQ",
          "type": "video",
          "caption": "Audeince interacting with light prop to change the bloom effect"
        },
        {
          "link": "https://www.youtube.com/embed/a1ziQecXQ6E",
          "type": "video",
          "caption": "Audience interaction"
        },
        {
          "link": "https://www.youtube.com/embed/6ozJphPfe3A",
          "type": "video",
          "caption": "Audience interaction"
        },
        {
          "link": "../images/toWater/3.jpg",
          "type": "image",
          "caption": "Audience using the light prop"
        },
        {
          "link": "../images/toWater/2.png",
          "type": "image",
          "caption": "File screenshot during installation"
        },
        {
          "link": "https://www.youtube.com/embed/YPlURdo0zrA",
          "type": "video",
          "caption": "File recording during installation"
        },
        {
          "link": "https://www.youtube.com/embed/kyYaKKzqk20",
          "type": "video",
          "caption": "Output capture during installation"
        },
        {
          "link": "../images/toWater/4.jpg",
          "type": "image",
          "caption": "Early prototype of the hardware setup"
        },
        {
          "link": "../images/toWater/5.png",
          "type": "image",
          "caption": "Screen capture of custom point cloud plant component made in TouchDesigner"
        }
      ]
    },
    {
      "name": "FAT32 Loss Protocol",
      "tags": [
        "hardware",
        "electronics",
        "interconnected",
        "immersive",
        "technology",
        "new media",
        "arts",
        "performance",
        "identity",
        "memory"
      ],
      "date": "October - November 2023",
      "technology": "TouchDesigner, Arduino, GLSL",
      "description": [
        {
          "text": "\"Do you remember everything that has ever happened to you?\" / FAT32 Loss Protocol is an AV performance project that explores the degeneration of memory generated by trauma by virtue of taking autonomy back over this process. The performance involves me drilling and hammering internal hard drives which glitches up memories of my childhood on my screen and transitions between them as one gets “successfully corrupted”."
        },
        {
          "header": "Technical Implementation",

          "text": "The project involves a wide pressure sensor connected to the hard drive being hammered and a small pressure sensor connected to the drill’s trigger. Upon pressing the drill, the value of the “datamosh variable” in the shader increments in a TouchDesigner GLSL patch. Afterwards, when I realize that I have successfully glitched out this memory, I start hammering the other hard drive which starts generating a noised tiled pattern whose threshold increases more towards white as I keep hammering it. Upon several hammer strokes, the noise pattern becomes fully white bringing out the new memory on screen and resetting the datamosh variable so I can continue corrupting the new memory."
        }
      ],
      "media": [
        {
          "link": "../images/fat32/1.jpg",
          "type": "image",
          "caption": "Performance capture 1"
        },
        {
          "link": "https://www.youtube.com/embed/YyDWmZYocxg",
          "type": "video",
          "caption": "Full Performance"
        },
        {
          "link": "https://www.youtube.com/embed/Xo62KMSpHxU",
          "type": "video",
          "caption": "Screen capture during performance"
        },
        {
          "link": "../images/fat32/2.jpg",
          "type": "image",
          "caption": "Performance capture 2"
        },
        {
          "link": "https://www.youtube.com/embed/KRRFWpQsM4M",
          "type": "video",
          "caption": "Performance highlight 1"
        },
        {
          "link": "https://www.youtube.com/embed/dCoe-ContOE",
          "type": "video",
          "caption": "Performance highlight 2"
        },

        {
          "link": "https://www.youtube.com/embed/9_fkD_fJs9c",
          "type": "video",
          "caption": "Performance highlight 3"
        },
        {
          "link": "https://www.youtube.com/embed/z4vzssLRLSs",
          "type": "video",
          "caption": "Performance highlight 4"
        },
        {
          "link": "../images/fat32/3.jpg",
          "type": "image",
          "caption": "All tools used for performance"
        }
      ]
    },
    {
      "name": "Synapse",
      "tags": [
        "interconnected",
        "immersive",
        "visualizers",
        "technology",
        "new media",
        "arts",
        "identity",
        "memory",
        "web",
        "game engine"
      ],
      "date": "December 2023",
      "technology": "Unity, HLSL",
      "sourceCode": "https://github.com/AakSin/shaderLabFall2023/blob/master/Assets/topics/Final/cave.compute",
      "description": [
        {
          "text": "Synapse was initially a Unity-based HLSL raymarching shader that is now a work in progress exploration of a webGL-based experimental social networking site. The initial conception of this project began with exploring an infinite cave system through setting up a raymarching shader but then it evolved into a more fleshed-out parallelism of the neuron systems in our brains. The visual involves the user making their way through infinite, generative tunnel systems that are constantly rotating, forming, and relaying messages between each other through the use of globular particles."
        },
        {
          "header": "Technical Implementation",

          "text": "The code is split between a computer shader and a Unity script file. The unity script file is used to transfer game camera information into the shader. The scene then traverses inside a raymarched gyroid. The gyroid’s z-axis is rotated over time to generate a pulsating feeling to the tunnels. A further noise-based Voronoi shader is applied to both the normals and the textures creating a neuron-like surface. This Voronoi equation is then applied in minimum with the rotating gyroid, generating the movement of particles over time. To make the particles more globular and make them appear as if they are emerging from or being absorbed by the walls. the smoothing coefficient of the smooth minimum function is manipulated over time as well. "
        }
      ],
      "media": [
        {
          "link": "../images/synapse/1.png",
          "type": "image",
          "caption": "Scene view capture of Synapse"
        },
        {
          "link": "https://player.vimeo.com/video/910363460",
          "type": "video",
          "caption": "Synapse final gameplay footage"
        },
        {
          "link": "https://www.youtube.com/embed/LPnDzIzAjrY",
          "type": "video",
          "caption": "Synapse testing / WIP gameplay footage"
        },
        {
          "link": "https://www.youtube.com/embed/b8kTym_9fws",
          "type": "video",
          "caption": "Synapse testing / WIP gameplay footage"
        }
      ]
    },
    {
      "name": "Abandoned Hotels of Zangsti",
      "tags": [
        "sound",
        "music",
        "visualizers",
        "technology",
        "new media",
        "arts",
        "identity",
        "memory",
        "game engine"
      ],
      "date": "September - October 2023",
      "technology": "Unreal Engine, Cineam4D, RedShift",
      "description": [
        {
          "text": "Set to “Abandoned Hotels of Zangsti” by Indian sound artist Ruhail Qaisar, the visual aims to be an extension of the psycho-ecological decay introduced by the soundscape. Through the haunting drones brought forward from the exploration of the slow loss of his hometown, I bring forward imagery of a frigid mountainous landscape going through deterioration and decay.  A descent into a foggy lake with fossils and lotuses, a first-person experience of disassociation of the land beneath, and a closing shot of the gradual collapse of the landmarks, work together to tell this story of postcolonial decay."
        },
        {
          "header": "Technical Implementation",

          "text": "The lake is set up through Unreal Engine’s fluid system and the fossils float on it using the parameters provided for setting up buoyancy. The disassociation of the land beneath the character happens by adding a Niagara particle system around the static mesh of the land. The final shot of the buildings being destroyed is generated through the chaos system provided by Unreal and then the debris along with the smoke is the result of Niagara systems attached to the chaos clusters."
        }
      ],
      "media": [
        {
          "link": "../images/abandonedHotels/particle.png",
          "type": "image",
          "caption": "Disinteragation exploration 1 in Cinema4D + Redshift (particle)"
        },
        {
          "link": "https://player.vimeo.com/video/871618358?h=4a586d07e",
          "type": "video",
          "caption": "Final Unreal Engine render"
        },
        {
          "link": "../images/abandonedHotels/cave.png",
          "type": "image",
          "caption": "Cave light render in Cinema4D + Redshift"
        },

        {
          "link": "../images/abandonedHotels/liquid.png",
          "type": "image",
          "caption": "WIP shot of disinteragation exploration 2 in Cinema4D (liquid)"
        },
        {
          "link": "../images/abandonedHotels/caveLight.png",
          "type": "image",
          "caption": "Cave light search in Cinema4D"
        },
        {
          "link": "https://www.youtube.com/embed/xTM8qp4cY8M",
          "type": "video",
          "caption": "Early failed buoyancy tests in Unreal Engine"
        },
        {
          "link": "../images/abandonedHotels/caveFragment.png",
          "type": "image",
          "caption": "WIP shot of cave in Cinema4D"
        },
        {
          "link": "../images/abandonedHotels/fragment.png",
          "type": "image",
          "caption": "Disinteragation exploration 2 in Cinema4D + Redshift (fragments)"
        },
        {
          "link": "../images/abandonedHotels/wipUE.png",
          "type": "image",
          "caption": "Early environment set-up in Unreal Engine"
        },
        {
          "link": "../images/abandonedHotels/wipFragment.png",
          "type": "image",
          "caption": "WIP shot of disinteragation exploration 2 in Cinema4D (particle)"
        }
      ]
    },
    {
      "name": "In Loving Memory Of",
      "tags": [
        "sound",
        "music",
        "immersive",
        "technology",
        "new media",
        "arts",
        "performance",
        "identity",
        "memory",
        "game engine"
      ],
      "date": "October - November 2023",
      "technology": "Unreal Engine, Ableton, Max MSP",
      "description": [
        {
          "text": "In Loving Memory is a live A/V performance piece created for an eponymous original track using Unreal Engine, Ableton, and MaxMSP. The visuals are an immersive environment extension of the sonic and lyrical elements in the song, to bring about a cohesive experience for the audience depicting a windy, stormy, and chaotic shore scene synced to the music."
        },
        {
          "header": "Technical Implementation",

          "text": "The live performance was broken down into three acts - setting up the background/ambiance using a heavily delayed guitar, playing the lead guitar on top of it, and doing vocals to bring the performance to an end. I set up an equivalent narrative progression for each of these acts in Unreal Engine then. Using the envelope follower in Ableton on each of the individual channels, sending that through a Max UDPsend node, and then receiving it in Unreal Engine using a UDP server blueprint."
        },
        {
          "text": "The background guitar influences the water dynamics, and upon reaching certain thresholds the water would get more turbulent. This was done by setting up a mobile invisible collision object in a Niagara fluids pool system. Then the lead guitar maps to a Niagara debris system and affects the spawn rate of the debris particle, amping them up on certain frequencies. The vocals then map to the lightning, triggering flashes in the weather system whenever my voice crosses a certain threshold."
        }
      ],
      "media": [
        {
          "link": "../images/inLoving/1.jpg",
          "type": "image",
          "caption": "Image from performance"
        },
        {
          "link": "https://www.youtube.com/embed/5LKl7OtqcDE",
          "type": "video",
          "caption": "Full Performance"
        },
        {
          "link": "https://www.youtube.com/embed/vya_bEZ0-lI",
          "type": "video",
          "caption": "Screen capture during performance"
        },

        {
          "link": "https://www.youtube.com/embed/ZEVd9IJEzLg",
          "type": "video",
          "caption": "Background Guitar"
        },
        {
          "link": "https://www.youtube.com/embed/7gMTC8qUh80",
          "type": "video",
          "caption": "Lead Guitar"
        },

        {
          "link": "https://www.youtube.com/embed/Wh3zhIBbjiA",
          "type": "video",
          "caption": "Vocals"
        },
        {
          "link": "https://www.youtube.com/embed/lBVQ5n7peAc",
          "type": "video",
          "caption": "First test performance"
        },
        {
          "link": "../images/inLoving/2.png",
          "type": "image",
          "caption": "Early environment render"
        },
        {
          "link": "https://www.youtube.com/embed/nndjatKuYmQ",
          "type": "video",
          "caption": "Early environment screen capture"
        }
      ]
    },

    {
      "name": "PXE",
      "tags": [
        "sound",
        "music",
        "visualizers",
        "technology",
        "new media",
        "arts",
        "identity",
        "memory"
      ],
      "date": "October - November 2023",
      "technology": "Maya, Arnold, Adobe After Effects, Adobe Premiere Pro",
      "description": [
        {
          "text": "\"Turn inside out to the outside in. Turn inside out to the outside within\". Building up on Swedish artist Ecco2k's work, this sequence aims to keep exploring themes of dissonance, fragmentation of self, and true identity/identities. The lyricism and sonic elements present in Ecco2k’s oeuvre prompted me to create a dissonant and harsh sequence of blinding violent lights, surreal landscapes, and deteriorating bodies. By melding these experimental visuals with Ecco2k's compelling sonic universe, I aimed to keep building up on previous and present works that interweave identity, memory, and deterioration."
        },
        {
          "header": "Technical Implementation",

          "text": "The first sequence of the lights swinging from the rooftop utilizes Maya’s system of MASH networks coupled with nHair and nConstraints. Each string hanging from the top is then provided some random turbulence and the camera orbits this room in swift movements. The landscape of electric towers involves creating a material for the surface that utilizes noise to intersperse a highly reflective surface (the puddles) with the alien terrain. The final shot of the barbed wire entity unraveling is animated using blend shapes in Maya."
        }
      ],
      "media": [
        {
          "link": "../images/pxe/2.png",
          "type": "image",
          "caption": "Environment scene still"
        },
        {
          "link": "https://player.vimeo.com/video/883771985",
          "type": "video",
          "caption": "Final render"
        },
        {
          "link": "../images/pxe/3.jpg",
          "type": "image",
          "caption": "Light scene still"
        },

        {
          "link": "../images/pxe/1.jpg",
          "type": "image",
          "caption": "Detanglement scene still"
        },
        {
          "link": "https://www.youtube.com/embed/UZPrLEAJa_g",
          "type": "video",
          "caption": "WIP playblast"
        },
        {
          "link": "../images/pxe/4.PNG",
          "type": "image",
          "caption": "WIP light scene / early render"
        },
        {
          "link": "../images/pxe/5.PNG",
          "type": "image",
          "caption": "Mesh shapes for detanglement scene"
        },
        {
          "link": "../images/pxe/6.PNG",
          "type": "image",
          "caption": "WIP environment scene set-up / early render"
        }
      ]
    },
    {
      "name": "Is It Cold In The Water",
      "tags": [
        "sound",
        "music",
        "arts",
        "audio reactive",
        "visualizers",
        "technology",
        "new media",
        "arts",
        "identity",
        "memory"
      ],
      "date": "November - December 2023",
      "technology": "Maya, Bitfrost, Arnold",
      "description": [
        {
          "text": "Inspired by an anecdote of a friend equating her trans existence to that of Medusa’s, this piece set to Sophie’s homonymous song continues towards a spiraling exploration of fluidity - both corporeal and spiritual. Visual cues like fragmentation and dissolution are brought about as allusions to alienation, subjugation, and detachment. These themes are countered by conceptual reversal with the dissolved bodies coming back together or breaking free. A shaky, fragile and pulsating environment houses these fluid bodies, as they echo the constant blindness of the ever-approaching enveloping unknown of the vocalist."
        },
        {
          "header": "Technical Implementation",

          "text": "The fluid simulation leverages Maya’s BitFrost fluid system. The arrows act as colliders while the bust of the statue acts as a motion field, which upon triggering towards the end brings the dispersed liquids back together. The particles are generated through Maya’s nParticle system in a spiraling pattern. They are provided with gradually increasing turbulence and upon the opening of her chest, the turbulence increases exponentially letting them escape out."
        }
      ],
      "media": [
        {
          "link": "../images/isItCold/2.jpg",
          "type": "image",
          "caption": "Liquid bust after re-assembling"
        },
        {
          "link": "https://player.vimeo.com/video/893127411",
          "type": "video",
          "caption": "Final render"
        },
        {
          "link": "../images/isItCold/4.jpg",
          "type": "image",
          "caption": "Statue environment still"
        },

        {
          "link": "../images/isItCold/1.jpg",
          "type": "image",
          "caption": "Collider arrow piercing liquid bust"
        },
        {
          "link": "../images/isItCold/3.jpg",
          "type": "image",
          "caption": "Particles escaping after statue opens"
        },
        {
          "link": "../images/isItCold/5.png",
          "type": "image",
          "caption": "WIP shot of setting up statue environment"
        }
      ]
    },

    {
      "name": "Electronicos Fantasticos",
      "tags": [
        "hardware",
        "electronics",
        "music",
        "sound",
        "technology",
        "performance",
        "arts"
      ],
      "date": "July - August 2022",
      "technology": "Hardware, Electronics, CRTs, Amplifiers",
      "description": [
        {
          "text": "After being awarded the NYU Abu Dhabi summer internship grant, I set out to Tokyo to work with Ei Wada and his Electronicos Fantasticos project. Electronicos Fantasticos is a project that aims “to reincarnate old electrical appliances as new electromagnetic instruments, invents new ways to play music, and co-create orchestras and festivals with diverse people and artist/musician Ei Wada.” I helped the team to perform at the Fuji Rock’ 22 Music Festival as well."
        },
        {
          "text": "Initially we were preparing to perform a version of the Bon Odori dance at the Fuji Rock festival. My work involved assisting with all the electronics at this time and there were instances I would also improvise on music compositions with the team. For Fuji Rock, we traveled with all our equipment from Tokyo to Naeba Ski Resort, in the Niigata prefecture of Japan. As our performance was late at night, we spent quite a bit of the evening setting up for the performance. "
        },
        {
          "text": "It was a great and learning experience helping out with the preparation for Fuji Rock. After the team got done with Fuji Rock, I got involved more in the technical aspects of the project and started creating instruments like the barcode scanner or the electric fan harp. Even after leaving Japan, I have still been in touch with Wada-san and the rest of the team. We have also set-up a “WORLDWIDE Orchest Lab” to bring together musicians globally who are interested in creating instruments like the ones Electronicos Fantasticos makes. "
        }
      ],
      "media": [
        {
          "link": "../images/nicos/nicos1.png",
          "type": "image",
          "caption": "Us preparing for a Bon Odori performance at Fuji Rock"
        },
        {
          "link": "../images/nicos/nicos2.png",
          "type": "image",
          "caption": "A meeting at the shrine to combine tradtional instruments with electornic"
        },
        {
          "link": "https://www.youtube.com/embed/m4FkYmwpMYE",
          "type": "video",
          "caption": "A short improvisation with me on the Electric Fan Harp"
        },
        {
          "link": "../images/nicos/nicos3.png",
          "type": "image",
          "caption": "Creating a barcode scanner instrument"
        },
        {
          "link": "https://www.youtube.com/embed/uip466i8Kec",
          "type": "video",
          "caption": "Playing the barcode scanner I created"
        },
        {
          "link": "../images/nicos/nicos4.jpg",
          "type": "image",
          "caption": "A CRTelecaster we worked on"
        },
        {
          "link": "https://www.youtube.com/embed/OakYFQBiFcY",
          "type": "video",
          "caption": "Exploring the sounds of a new electronic device with Wada-san"
        },
        {
          "link": "../images/nicos/nicos5.png",
          "type": "image",
          "caption": "Experimenting with the Electric Fan Harp's light system"
        },
        {
          "link": "https://www.youtube.com/embed/M-05hiOEqC8",
          "type": "video",
          "caption": "Setting up before the Fuji Rock performance"
        },
        {
          "link": "https://www.youtube.com/embed/IqBaU5mhl2I",
          "type": "video",
          "caption": "Combining both skates and barcodes together"
        }
      ]
    },
    {
      "name": "Faceshopping",
      "tags": [
        "augmented reality",
        "audio reactive",
        "music",
        "technology",
        "new media",
        "identity",
        "transmedia",
        "visualizers"
      ],
      "date": "December 2022",
      "technology": "Cinema4D, Meta Spark Studio",
      "sourceCode": "https://github.com/AakSin/Faceshopping",
      "liveLink": "https://www.instagram.com/ar/1794320447605221/",
      "description": [
        {
          "text": "Faceshopping is an instagram filter that recreates SOPHIE’s music video for Faceshopping in augmented reality. The music video covers themes of body dysmorphia especially with the lens of SOPHIE being a transgender individual. By turning the camera at the user, I aim to put the user in SOPHIE’s place. The filter was presented in Berlin, Germany at “MANIFEST:IO – The Symposium for New Media & Electronic Art.”"
        },
        {
          "header": "Conception",
          "text": "SOPHIE’s Faceshopping music video has stuck with me ever since I saw it. It contains visuals of SOPHIE’s face being distorted while loud abrasive hyperpop/PC-music blares through. For this project, I wanted people to be able to experience these distortions and music on their face. I looked into a lot of AR platforms like Unity, Vuforia or Styly but realized Meta had created a pretty good solution for face filters."
        },
        {
          "header": "Technical Implementation",
          "text": "I imported the model of the face that Meta provides for the filter into Cinema4D. I added a PoseMorph tag to it and then set up keyframes with 6 different distortions. I exported this model into Meta Spark Studio. I used the frequency splitter node in Spark AR’s node graph then and passed SOPHIE”s Faceshopping through it. The middle frequencies were mapped to the intensity of the light and the back wall. The high frequencies were mapped to the direction and movement of the light. I wrote a simple javascript script after that to animate and cycle through all the distortions periodically and in a loop."
        },
        {
          "header": "Exhibition",
          "text": "The filter was part of TimeLab’s, a transmedia laboratory for artistic research in Berlin, MANIFEST:IO Symposium for New Media and Electronic Art in Berlin. I printed the poster pictured above and hung it around the venue. The exhibition occurred at Alte Muenze, a popular night club and an art venue in Berlin."
        }
      ],
      "media": [
        {
          "link": "../images/faceshopping/1.png",
          "type": "image",
          "caption": "The poster for Faceshopping at the MANIFEST:IO exhibition"
        },
        {
          "link": "https://www.youtube.com/embed/_eCXYYLGoY0",
          "type": "video",
          "caption": "A demo of the filter on my face"
        },
        {
          "link": "../images/faceshopping/2.png",
          "type": "image",
          "caption": "Cinema4D project file screenshot"
        },
        {
          "link": "../images/faceshopping/3.png",
          "type": "image",
          "caption": "Meta Spark Studio file screenshot"
        },
        {
          "link": "../images/faceshopping/4.png",
          "type": "image",
          "caption": "A still from the Faceshopping music video by SOPHIE"
        },
        {
          "link": "../images/faceshopping/5.png",
          "type": "image",
          "caption": "An in-app render of the filter"
        }
      ]
    },
    {
      "name": "aloegarten",
      "tags": [
        "virtual reality",
        "music",
        "new media",
        "immersive",
        "transmedia",
        "technology",
        "arts",
        "identity",
        "visualizers",
        "game engine"
      ],
      "date": "October - November 2022",
      "technology": "Unity, C#, Cinema4D, Oculus",
      "description": [
        {
          "text": "Aloegarten is a virtual reality recreation of the worlds of Swedish artist Ecco2k / Zak Arogundade. Themes from his works regarding colorist societal alienation, substance abuse, powerlessness and identity are extrapolated in this three dimensional transmedia project.The project covers two of his songs - Peroxide and AAA Powerline."
        },
        {
          "header": "Conception",
          "text": "The themes present in Ecco2k’s works are very personal to my lives as well. Through an exploration of his music videos and lyricism, I chose to expand and interpret these themes into my understanding of them. This started out by creating conception sketches in Procreate (present in the gallery above). This was to gain an understanding of the feel of the scenes. Then I started mapping out the project in 3D."
        },
        {
          "header": "Technical Implementation",
          "text": "The project is created using Unity. For the water in the first/Peroxide scene, I use Unity’s Universal Render Pipeline (URP). Using displacement maps and shadergraphs, I was able to simulate wave-like motion for the water. I then wrote C# scripts that would track this motion and create a buoyant force on any rigid body floating on water. The raft in the scene was given this script along with a controller script. A portal was implemented to take the user to the second/AAA Powerline scene."
        },
        {
          "text": "In the AAA Powerline scene, I place the viewer into a car and create a cyberpunk city around them. I then start tweaking the material of the glass so that it simulates refraction and distortion of perspectives when viewed from different angles. The person is unable to move in the scene. They are in the backseat and unable to control their actions. I create a holographic material for the VR hands of the person, which they have been detached from. The car continues moving in an endless loop, unable to be controlled and having you forever stuck."
        }
      ],
      "media": [
        {
          "link": "../images/aloegarten/1.png",
          "type": "image",
          "caption": "A still from the Peroxide scene"
        },
        {
          "link": "../images/aloegarten/2.png",
          "type": "image",
          "caption": "A still from the AAA Powerline scene"
        },
        {
          "link": "https://www.youtube.com/embed/w7fjQeXgBbU",
          "type": "video",
          "caption": "A full walkthrough of aloegarten"
        },
        {
          "link": "../images/aloegarten/3.png",
          "type": "image",
          "caption": "Conception sketches for AAA Powerline scene"
        },
        {
          "link": "../images/aloegarten/4.png",
          "type": "image",
          "caption": "Conception sketches for a deleted scene"
        },
        {
          "link": "../images/aloegarten/5.png",
          "type": "image",
          "caption": "A screenshot of the shader graph for the water"
        },
        {
          "link": "../images/aloegarten/6.png",
          "type": "image",
          "caption": "A still from the Peroxide scene"
        },
        {
          "link": "../images/aloegarten/7.png",
          "type": "image",
          "caption": "A still from the AAA Powerline scene"
        }
      ]
    },

    {
      "name": "“Real” Art",
      "tags": ["technology", "installation", "artificial intelligence", "web"],
      "date": "November 2022",
      "technology": "JavaScript, React, Next.js, Firebase",
      "sourceCode": "https://github.com/AakSin/realArt",
      "liveLink": "https://louvread-realart.vercel.app/",
      "description": [
        {
          "header": "Context",
          "text": "“Real” Art was created in collaboration with Louvre Abu Dhabi and was part of a weekend installation there. The initial prompt provided to us was, “What does it mean to be human?”. This was a time when conversations regarding AI art, DALL-E, ownership etc. were quite prevalent. In the context of this situation, we decided to create a project that would explore whether people could even sense the presence of a “real” human artist in artworks anymore."
        },
        {
          "header": "Concept",
          "text": "The concept revolves around presenting two images to a user side-by-side. One of them present at the Louvre Abu Dhabi as of now and another that was generated by an AI image generator (DALL-E). After selecting their answers, the cards containing the image would flip. The AI card would reveal the prompt used to generate the image. The real card would reveal the name of the artwork, artist and year of creation. The users could also see a voting tally at the bottom, to see how they compared against other users."
        },
        {
          "header": "AI Image Generation",
          "text": "This part of the project had some unexpected challenges. As seen in the initial renders in the screenshots above, the AI would have glitch artifacts, issues with human limbs or minor details in nature would be wrong. Researching the schools of art, the painters and their styles, the subjects etc. helped us get to prompts that would render a similar picture."
        },
        {
          "header": "Technical Implementation",
          "text": "The front end is created using Next.js. It was created to be displayed on a large touchscreen display along with multiple iPads spread across the site of install. The backend involves a Firebase storage saving the number of votes each painting has garnered along with the rest of the painting’s metadata. There are then API calls being done using Next.js to handle retrieval and incrementation of votes."
        },
        {
          "header": "Exhibition",
          "text": "After installing the project in the Louvre Abu Dhabi premises, we started inviting people to take the quiz. We’d explain what the project is, what AI art is and then let people take the quiz. Throughout the quiz and after the quiz we’d generate discussions among groups of people regarding how they are telling the AI artwork apart from the real artwork, the ethics of AI art and what does the future of the art landscape look like with the introduction of AI."
        }
      ],
      "media": [
        {
          "link": "../images/realart/1.png",
          "type": "image",
          "caption": "Screenshot of project website"
        },
        {
          "link": "../images/realart/2.png",
          "type": "image",
          "caption": "Renders by the AI"
        },
        {
          "link": "../images/realart/3.png",
          "type": "image",
          "caption": "Renders by the AI"
        },
        {
          "link": "https://www.youtube.com/embed/xePH6z5mIQ8",
          "type": "video",
          "caption": "Overview of \"Real\" Art Installation Setup"
        },
        {
          "link": "https://www.youtube.com/embed/psr9Tbsioxw",
          "type": "video",
          "caption": "Audience Interaction at \"Real\" Art"
        },
        {
          "link": "https://www.youtube.com/embed/16X2hZOTZp0",
          "type": "video",
          "caption": "Audience Interaction at \"Real\" Art"
        }
      ]
    },
    {
      "name": "Komposition",
      "tags": ["augmented reality", "technology", "new media", "game engine"],
      "date": "March - May 2022",
      "technology": "Unity, C#",
      "sourceCode": "https://github.com/AakSin/komposition",
      "description": [
        {
          "text": "Komposition is an AR app that allows you to visualize German compound words in real space. The project is a surrealist take at exploring the absurdities inherent in our colloquial language. Using Komposition users can combine two german words, see a physical representation of this fusion and then place this word around them."
        },
        {
          "header": "Conception",
          "text": "The app started out as a project to investigate how one can visualize collections of objects that aren’t real. My teammate and I, during our German classes, started noticing the amount of compound words that exist in German. This is a subject that amuses both natives and new learners. With the lack of spaces in the middle and the ability to chain multiple words together, German compound words reach indefinable lengths. The absurdity of this phenomenon made it the final choice for our collection."
        },
        {
          "header": "Technical Implementation",
          "text": "The app is implemented using Unity’s AR foundation framework. A material has been created for word 1 and a model has been assigned for word 2. The program dynamically instantiates an object by combing the model and material, depending on the inputs selected. The user then gets to place this object around them by tapping the screen on the planes that have been detected by Unity. Using raycast collisions, a touch input on the object is detected again, allowing the user to edit (move, rotate, resize) the object."
        },
        {
          "text": "Using Unity’s PlayerPrefs we keep a store of all the objects the user has unlocked so far. The user can then go and access this compendium again. They can track their progress in unlocking all objects, read more about the objects they have already unlocked and place them in the real world again from this page."
        }
      ],
      "media": [
        {
          "link": "../images/komposition/1.png",
          "type": "image",
          "caption": "Hologram before object gets placed"
        },
        {
          "link": "https://www.youtube.com/embed/kXxEMpsDIZ0",
          "type": "video",
          "caption": "A full working demo of the app"
        },
        {
          "link": "../images/komposition/2.png",
          "type": "image",
          "caption": "Screen for combining words"
        },
        {
          "link": "../images/komposition/3.png",
          "type": "image",
          "caption": "Work in progress shot while testing object instantiation on tap"
        },
        {
          "link": "../images/komposition/4.png",
          "type": "image",
          "caption": "A screenshot of the Figma file"
        }
      ]
    },
    {
      "name": "Cryoponics",
      "date": "June-July 2023",
      "technology": "Cinema4D, RedShift, Adobe AfterEffects",
      "tags": ["arts", "technology", "music", "new media"],

      "description": [
        {
          "text": "Cryoponics is a visual exploration of the evolution of organic matter from seemingly non-organic sources. Inspired by the idea of hydroponics used for the growth of plant-based material in space, I set out to create an equal parts alien and equal parts natural flora portraiture."
        },
        {
          "header": "Technical Implementation",
          "text": "The surreal “florality” is achieved by taking pre-existing 3D scans of organic matter like grass and then dispersing them through random variations on a branch. Then color ramps are added to each variation, generating floral shades of alien nature. Then an ice material was created by altering parameters like sub-surface scattering. A melt deformer is applied on the iceblock and color values are changed throughout the animation to calibrate/maintain the alien nature of the visual palette."
        }
      ],
      "media": [
        {
          "link": "../images/3Drenders/cryoponics/blue-2.jpg",
          "type": "image",
          "caption": "Stills of cryoponic flora"
        },

        {
          "link": "https://www.youtube.com/embed/Q6qU-XivToU",
          "type": "video",
          "caption": "Cryoponic growth"
        },
        {
          "link": "../images/3Drenders/cryoponics/red.jpg",
          "type": "image",
          "caption": "Stills of cryoponic flora"
        },
        {
          "link": "../images/3Drenders/cryoponics/vid-2.jpg",
          "type": "image",
          "caption": "Cryoponic flora emerging out"
        },

        {
          "link": "../images/3Drenders/cryoponics/yellow.jpg",
          "type": "image",
          "caption": "Stills of cryoponic flora"
        }
      ]
    },
    {
      "name": "CamJam",
      "tags": ["technology", "sound", "music", "interconnected", "web"],
      "date": "May 2022",
      "technology": "JavaScript, ml5.js,p5.js,WebSockets",
      "sourceCode": "https://github.com/AakSin/CamJam",
      "liveLink": "https://camjam1.glitch.me/",
      "description": [
        {
          "text": "CamJam is a web-based multiplayer platform wherein users can play instruments like the piano, guitar, bass and drums just by using their webcam. A person can create a room and then share that link with up to 3 more people. Everyone can then select their instruments and start jamming together."
        },
        {
          "header": "Concept",
          "text": "During COVID-19, I came to realize how much I missed playing music together with my friends. I initially started out looking for a way to jam with real instruments over the web but then I started thinking about people who don’t own instruments. With this project, I set out to provide a platform where people could jam together with their friends by just using their webcams (something that became a necessity during COVID’s Zoom era)."
        },
        {
          "header": "Instrument Implementation",
          "text": "The piano works on ml5.js’s HandPose model. It gets the coordinate point of each joint in the hand and then depending on which finger was brought down, plays a certain note. The bass and drums work on ml5.js’s PoseNet model. Depending on how much movement the left and right wrists had in certain sections of the screen, a certain sound is played. The guitar combines both HandPose and PoseNet. The left hand keeps track of which finger is down and which note to play. The right hand tracks the wrist to check when a person brings it down, which equates to a strum."
        },
        {
          "header": "Server Implementation",
          "text": "The site uses p5’s Live Media implementation (https://github.com/vanevery/p5LiveMedia). The  user’s webcam is captured using the createCapture method. The other webcams are provided as streams and are “drawn onto canvas”. To pass sound information around, I used the provided getData method. Depending on whichever sound had to be played, a certain string would be passed around. This would trigger the sound in all clients except the creator of the sound."
        }
      ],
      "media": [
        {
          "link": "../images/camjam/1.png",
          "type": "image",
          "caption": "Screenshot of website layout"
        },
        {
          "link": "../images/camjam/2.gif",
          "type": "image",
          "caption": "Guitar tech demo"
        },
        {
          "link": "https://www.youtube.com/embed/d5lLMOwFY3A?si=d21CdS43a5gb1V5W",
          "type": "video",
          "caption": "Working demo of drums and guitar"
        },
        {
          "link": "../images/camjam/3.gif",
          "type": "image",
          "caption": "Piano tech demo"
        },
        {
          "link": "../images/camjam/4.gif",
          "type": "image",
          "caption": "Drum tech demo"
        },
        {
          "link": "../images/camjam/5.gif",
          "type": "image",
          "caption": "Bass tech demo"
        },
        {
          "link": "https://www.youtube.com/embed/2_LWjRFyWSs?",
          "type": "video",
          "caption": "In-person demo of CamJam"
        }
      ]
    },
    {
      "name": "Genesis",
      "tags": [
        "technology",
        "virtual reality",
        "music",
        "hardware",
        "electronics",
        "new media",
        "visualizers"
      ],
      "date": "April-May 2022",
      "technology": "A-Frame, three.js, JavaScript, Arduino, C++, XBee, WebSockets",
      "sourceCode": "https://github.com/AakSin/IntroToIM/tree/main/finalProject",
      "liveLink": "https://aframesockets.glitch.me/",
      "description": [
        {
          "text": "Genesis is a webVR music experience set to Genesis by Grimes. The experience involves the user suspended in mid-air as particles revolve around them. With a custom Arduino controller made by us, the user could manipulate these particles and move around. The aim was to create a surreal and psychedelic visualizer to Grimes’ track while providing the user complete immersion and control."
        },
        {
          "text": "Note: The live demo is paired with a custom Arduino controller to control the visuals. Even without it though, a user can still enjoy the visuals in VR."
        },
        {
          "header": "Hardware Implementation",
          "text": "The controller involves an Arduino Uno housing an accelerometer, two buttons and an XBee module. Since the visuals of the project involve three dimensions, we wanted a sensor that would also give us inputs in three dimensions. Hence, we chose the accelerometer to give us the user’s tilt and motion in the three axes. The buttons are simply to move up and down in the virtual world. The XBee module was crucial to make the controller wireless. Initial prototypes involved the project with a wire and that would severely restrict the controller’s motion."
        },
        {
          "header": "Website Implementation",
          "text": "The virtual world is set up using A-Frame, a webVR framework built on top of three.js. I set up an animation for the spheres to move in a spiral pattern by iterating through all of them and changing their transformations. The input from the controller then controls the radius of this spiral, the movement speed of the spheres and the color of the spheres. Using spatial audio, as the user descends down into the spiral, the audio of Genesis becomes louder."
        },
        {
          "text": "As we didn’t own a virtual reality headset at that time, we decided to use a phone inside of Google Cardboard as our virtual reality headset. The XBee module only connects with a computer though and thus I had to set up an entire WebSocket system to communicate data between the phone and the laptop. Upon every change of input received by the laptop, the data would be sent to all other clients who have the site open. This included the phone, hence allowing the phone to mirror the laptop’s visuals."
        }
      ],
      "media": [
        {
          "link": "../images/genesis/1.gif",
          "type": "image",
          "caption": "Genesis Device Demo"
        },
        {
          "link": "https://www.youtube.com/embed/ui-yJ7_DYvo",
          "type": "video",
          "caption": "In-class presentation of Genesis"
        },
        {
          "link": "https://www.youtube.com/embed/ELKH0ijsJAQ",
          "type": "video",
          "caption": "Exhibition presentation of Genesis"
        },
        {
          "link": "https://www.youtube.com/embed/u7Tf66VZSUg",
          "type": "video",
          "caption": "Prototyping Genesis V1"
        },
        {
          "link": "https://www.youtube.com/embed/jX-XVjoRggI",
          "type": "video",
          "caption": "Prototyping Genesis V2"
        },
        {
          "link": "../images/genesis/2.jpg",
          "type": "image",
          "caption": "Picture of Genesis Controller"
        }
      ]
    },

    {
      "name": "More Work...",
      "tags": [
        "technology",
        "audio reactive",
        "music",
        "new media",
        "arts",
        "visualizers"
      ],

      "technology": "Blender, Cinema4D, RedShift, TouchDesigner, Shaders, Unreal Engine",
      "description": [
        {
          "text": "Collection of miscellaneous projects"
        }
      ],
      "media": [
        {
          "link": "../images/3Drenders/displacement-map/temple-render.jpg",
          "type": "image",
          "caption": "Still from Fatima"
        }
      ]
    }
  ]
}
